{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mini Project: Deep Learning with Keras\n",
        "\n",
        "In this mini-project we'll be building a deep learning classifier using Keras to predict income from the popular [Adult Income dataset](http://www.cs.toronto.edu/~delve/data/adult/adultDetail.html).\n",
        "\n",
        "Predicting income from demographic and socio-economic information is an important task with real-world applications, such as financial planning, market research, and social policy analysis. The Adult dataset, sometimes referred to as the \"Census Income\" dataset, contains a vast amount of anonymized data on individuals, including features such as age, education, marital status, occupation, and more. Our objective is to leverage this data to train a deep learning model that can effectively predict whether an individual's income exceeds $50,000 annually or not.\n",
        "\n",
        "Throughout this Colab, we will walk you through the entire process of building a deep learning classifier using Keras, a high-level neural network API that runs on top of TensorFlow. Keras is known for its user-friendly and intuitive interface, making it an excellent choice for both beginners and experienced deep learning practitioners.\n",
        "\n",
        "Here's a brief outline of what we will cover in this mini-project:\n",
        "\n",
        "1. **Data Preprocessing:** We will start by loading and exploring the Adult dataset.\n",
        "\n",
        "2. **Building the Deep Learning Model:** We will construct a neural network using Keras, where we'll dive into understanding the key components of a neural network, including layers, activation functions, and optimization algorithms.\n",
        "\n",
        "3. **Model Training:** With our model architecture in place, we will split the data into training and validation sets and train the neural network on the training data. We will monitor the training process to prevent overfitting and enhance generalization.\n",
        "\n",
        "4. **Model Evaluation:** After training, we'll assess the performance of our model on the test dataset.\n",
        "\n",
        "By the end of this tutorial, you will not only have a functional deep learning classifier for income prediction but also gain valuable insights into how to leverage the power of neural networks for solving real-world classification tasks.\n"
      ],
      "metadata": {
        "id": "fyXucUekO19i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "id": "rAGzXpBhHLPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLWR1DfQPakn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can download the Adult data from the link [here](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data).\n",
        "\n",
        "Here are your tasks:\n",
        "\n",
        "  1. Load the Adult data into a Pandas Dataframe.\n",
        "  2. Ensure the dataset has properly named columns. If the columns are not read in, assign them by referencing the dataset documentation.\n",
        "  3. Display the first five rows of the dataset."
      ],
      "metadata": {
        "id": "5ymxgnyq86hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "\n",
        "column_names = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
        "    'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
        "    'hours_per_week', 'native_country', 'income'\n",
        "]\n",
        "\n",
        "# Download the dataset and load it into a pandas DataFrame\n",
        "df = pd.read_csv(DATA_PATH, header=None, names=column_names, na_values=\" ?\", skipinitialspace=True)"
      ],
      "metadata": {
        "id": "QmwdQy7pShig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the DataFrame\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "X1wSIzVtPrfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're not already familiar with the Adult dataset, it's important to do some exploratory data analysis.\n",
        "\n",
        "Here are your tasks:\n",
        "\n",
        "  1. Do exploratory data analysis to give you some better intuition for the dataset. This is a bit open-ended. How many rows/columns are there? How are NULL values represented? What's the percentage of positive cases in the dataset?\n",
        "\n",
        "  2. Drop all rows with NULL values.\n",
        "\n",
        "  3. Use Scikit-Learn's [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) to convert the `income` column with a data type string to a binary variable."
      ],
      "metadata": {
        "id": "5fHLuKZl9ivm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do some exploratory analysis. How many rows/columns are there?\n",
        "print(\"Shape=\",df.shape)\n",
        "# How are NULL values represented?\n",
        "print('-----Sum of Null------')\n",
        "print(df.isnull().sum())\n",
        "# What's the percentrage of positive cases in the dataset?\n",
        "positive_cases =  df[df['income'] == '>50K']\n",
        "print('percentrage of positive =',(len(positive_cases)/len(df))*100,'%')"
      ],
      "metadata": {
        "id": "fc_s4kRKTloe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all NULL values and drop them\n",
        "df.dropna()"
      ],
      "metadata": {
        "id": "pZW7GRw3P0dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Scikit-Learn's LabelEncoder to convert the income column with a data type string to a binary variable.\n",
        "label_encoder = LabelEncoder()\n",
        "df['income_binary'] = label_encoder.fit_transform(df['income'])\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "BZ_mJT_DLZ-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X3qMPaRJzEZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Split the data into training and test sets. Remember not to include the label you're trying to predict, `income`, as a column in your training data."
      ],
      "metadata": {
        "id": "ibK0DxJsA1JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training and test sets\n",
        "X = df.drop(['income', 'income_binary'], axis=1)\n",
        "y = df['income_binary']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "1whzL6K7J-zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, the Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) metric are commonly used to evaluate the performance of binary classification models. These are valuable tools for understanding how well a model can distinguish between the positive and negative classes in a classification problem.\n",
        "\n",
        "Let's break down each concept:\n",
        "\n",
        "1. ROC Curve:\n",
        "The ROC curve is a graphical representation of a binary classifier's performance as the discrimination threshold is varied. It is created by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) at different threshold values. Here's how these rates are calculated:\n",
        "\n",
        "- True Positive Rate (TPR), also called Sensitivity or Recall, measures the proportion of actual positive instances that are correctly identified by the model:\n",
        "   TPR = True Positives / (True Positives + False Negatives)\n",
        "\n",
        "- False Positive Rate (FPR) measures the proportion of actual negative instances that are incorrectly classified as positive by the model:\n",
        "   FPR = False Positives / (False Positives + True Negatives)\n",
        "\n",
        "The ROC curve is useful because it shows how well a classifier can trade off between sensitivity and specificity across different threshold values. The ideal ROC curve hugs the top-left corner, indicating a high TPR and low FPR, meaning the classifier is excellent at distinguishing between the two classes.\n",
        "\n",
        "2. AUC (Area Under the Curve):\n",
        "The AUC is a scalar metric derived from the ROC curve. It represents the area under the ROC curve, hence its name. The AUC ranges from 0 to 1, where 0 indicates a very poor classifier (always predicting the opposite class) and 1 signifies a perfect classifier (making all correct predictions).\n",
        "\n",
        "The AUC metric is beneficial because it provides a single value to summarize the classifier's overall performance across all possible threshold values. It is particularly useful when dealing with imbalanced datasets, where one class significantly outnumbers the other. In such cases, accuracy alone might not be a reliable evaluation metric, and AUC can provide a more robust performance measure.\n",
        "\n",
        "A quick rule of thumb for interpreting AUC values:\n",
        "- AUC ≈ 0.5: The model performs no better than random guessing.\n",
        "- 0.5 < AUC < 0.7: The model has poor to fair performance.\n",
        "- 0.7 < AUC < 0.9: The model has good to excellent performance.\n",
        "- AUC ≈ 1: The model is close to or has a perfect performance."
      ],
      "metadata": {
        "id": "HMsXM6B_BX5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are your tasks:\n",
        "\n",
        "  1. Use Scikit-Learn's [roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) to calculate the AUC score for a method that always predicts the majority class.  "
      ],
      "metadata": {
        "id": "NDGgBVEKEYKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Scikit-Learn's roc_auc_score to calculate the AUC score for a method that always predicts the majority class.\n",
        "check_majority_class = df['income_binary'].value_counts()\n",
        "print(check_majority_class)\n",
        "\n",
        "majority_class  = y_test.mode()\n",
        "print('majority_class-->',majority_class)\n",
        "\n",
        "y_pred = [majority_class] * len(y_test)\n",
        "y_pred_prob = [0.5] * len(y_test)\n",
        "auc_score = roc_auc_score(y_test, y_pred_prob)\n",
        "print(f\"AUC score for a model always predicting the majority class: {auc_score}\")"
      ],
      "metadata": {
        "id": "s00Xs8bqUZnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's do a little feature engineering.\n",
        "\n",
        "1. Use Scikit-Learn's [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) to apply One Hot Encoding to the categorical variables in `workclass`, `education`, `marital-status`, `occupation`, `relationship`, 'race', `sex`, and `native-country`. Also, apply [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to the remaining continuous features. How many columns will the dataframe have after these columns transformations are applied?"
      ],
      "metadata": {
        "id": "uWSiYNarF2t_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Scikit-Learn's ColumnTransformer to apply One Hot Encoding to the categorical variables in workclass, education, marital-status, occupation,\n",
        "# relationship, 'race', sex, and native-country. Also, apply MinMaxScaler to the remaining continuous features.\n",
        "categorical_columns  = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex',  'native_country']\n",
        "continuous_columns = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
        "\n",
        "tr =[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns), ('num', MinMaxScaler(), continuous_columns)]\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=tr)"
      ],
      "metadata": {
        "id": "4DybgGJyW-3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many columns will the dataframe have after these columns transformations are applied?\n",
        "transformed_data = preprocessor.fit_transform(df)\n",
        "print(f\"number of columns = {transformed_data.shape[1]}\")\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "emfaqHwvKfLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras is an open-source deep learning library written in Python. It was developed to provide a user-friendly, high-level interface for building and training neural networks. The library was created by François Chollet and was first released in March 2015 as part of the Deeplearning4j project. Later, it became part of the TensorFlow ecosystem and is now the official high-level API for TensorFlow.\n",
        "\n",
        "Keras is designed to be modular, user-friendly, and easy to extend. It allows researchers and developers to quickly prototype and experiment with various deep learning models. One of the primary goals of Keras is to enable fast experimentation, making it simple to build and iterate on different architectures.\n",
        "\n",
        "Key features of Keras include:\n",
        "\n",
        "1. User-friendly API: Keras provides a simple and intuitive interface for defining and training deep learning models. Its design philosophy focuses on ease of use and clarity of code.\n",
        "\n",
        "2. Modularity: Models in Keras are built as a sequence of layers, and users can easily stack, merge, or create complex architectures using a wide range of predefined layers.\n",
        "\n",
        "3. Extensibility: Keras allows users to define custom layers, loss functions, and metrics. This flexibility enables researchers to experiment with new ideas and algorithms seamlessly.\n",
        "\n",
        "4. Backends: Initially, Keras supported multiple backends, including TensorFlow, Theano, and CNTK. However, as of TensorFlow version 2.0, TensorFlow has become the primary backend for Keras.\n",
        "\n",
        "5. Multi-GPU and distributed training: Keras supports training models on multiple GPUs and in distributed computing environments, making it suitable for large-scale experiments.\n",
        "\n",
        "6. Pre-trained models: Keras includes a collection of pre-trained models for common tasks, such as image classification (e.g., VGG, ResNet, MobileNet) and natural language processing (e.g., Word2Vec, GloVe).\n",
        "\n",
        "The integration of Keras into TensorFlow as its official high-level API has solidified its position as one of the most popular deep learning libraries in the machine learning community. Its ease of use and versatility have contributed to its widespread adoption in both academia and industry for a wide range of deep learning tasks."
      ],
      "metadata": {
        "id": "AtoqTz5rGuET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are your tasks:\n",
        "\n",
        "1. Create your own model in Keras to predict income in the Adult training data. Remember, it's always better to start simple and add complexity to the model if necessary. What's a good loss function to use?\n",
        "\n",
        "2. Keras can be integrated with Scitkit-Learn using a wrapper. Use the [KerasClassifier wrapper](https://adriangb.com/scikeras/stable/generated/scikeras.wrappers.KerasClassifier.html) to integrate your Keras model with the ColumnTransformer from previous steps using a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) object.\n",
        "\n",
        "3. Fit your model.\n",
        "\n",
        "4. Calculate the AUC score of your model on the test data. Does the model predict better than random?\n",
        "\n",
        "5. Generate an ROC curve for your model using [RocCurveDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html). What would the curve look like if all your predictions were randomly generated? What would the curve look like if it you had a perfect model?"
      ],
      "metadata": {
        "id": "HVUa0h83HU24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=14, activation=\"relu\"))\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(transformed_data.shape)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=5)\n",
        "\n",
        "\n",
        "# model.fit_transform(X_train, y_train, epochs=5)\n",
        "# test_loss, test_accuracy =  model.evaluate(X_test, y_test)\n",
        "# print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "h2xIpLlXQEcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Keras classifier\n",
        "\n",
        "# Use the calculated input_dim in the model definition\n",
        "def create_model():\n",
        "  model_two = Sequential()\n",
        "  model_two.add(Dense(64, input_dim=108, activation=\"relu\"))\n",
        "  model_two.add(Dense(32, activation=\"relu\"))\n",
        "  model_two.add(Dense(1, activation=\"sigmoid\"))\n",
        "  model_two.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model_two\n",
        "\n",
        "keras_model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)"
      ],
      "metadata": {
        "id": "Rz-m2LhrQGud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the scikit-learn pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', keras_model)\n",
        "])"
      ],
      "metadata": {
        "id": "VKxkil7QQJ6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the pipeline on the training data\n",
        "\n",
        "# Re-sample using from above\n",
        "# y = df['income_binary']\n",
        "# X_train, X_test, y_train, y_test = train_test_split(transformed_data, y, test_size=0.2, random_state=42)\n",
        "\n",
        "pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "25O8ZLleGQnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the AUC score of your model on the test data.\n",
        "# Does the model predict better than random?"
      ],
      "metadata": {
        "id": "SLcNQGVqNYbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an ROC curve for your model."
      ],
      "metadata": {
        "id": "prJG9pr7PYIc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}